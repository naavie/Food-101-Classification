{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbfybt0NBZHx"
      },
      "source": [
        "# Section 1: General EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lce8abh5gECs"
      },
      "outputs": [],
      "source": [
        "# General modules\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "# Image visualization modules\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "\n",
        "# ResNet/Model modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqTyZb_GgW_e"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVc0IHX6gXSP",
        "outputId": "5a362c17-3541-413c-d6cc-44fcd9408193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZkhp_gcgesq",
        "outputId": "faefa4fb-732c-45b7-e9d7-5a01b8612a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 101 image clasess in the food-101 dataset\n"
          ]
        }
      ],
      "source": [
        "images_path = '/content/drive/Othercomputers/My Computer/food-101/images'\n",
        "all_image_classes = os.listdir(images_path)\n",
        "print(\"There are {0} image clasess in the food-101 dataset\".format(len(all_image_classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHdUt0xHgjbK",
        "outputId": "dadacb29-8143-4af4-ead6-1969488041cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['train.txt', 'train.json', 'test.txt', 'test.json', 'labels.txt', 'classes.txt']\n"
          ]
        }
      ],
      "source": [
        "metadata_path = '/content/drive/Othercomputers/My Computer/food-101/meta'\n",
        "metadata_info = os.listdir(metadata_path)\n",
        "print(metadata_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vhyj8e3gn6w",
        "outputId": "fb272675-7059-4f3e-8ead-99c782ab804c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n",
            "\n",
            "101 food classes were found in the 'classes.txt' file in the 'meta' folder.\n"
          ]
        }
      ],
      "source": [
        "# 'classes.txt'\n",
        "meta_class_data = os.path.join(metadata_path, 'classes.txt')\n",
        "\n",
        "with open(meta_class_data, 'r') as f:\n",
        "    meta_classes = f.read().splitlines()  # Classes listed in the 'classes.txt' file in the 'meta' folder\n",
        "\n",
        "print(meta_classes)\n",
        "print(\"\")\n",
        "print(\"{0} food classes were found in the 'classes.txt' file in the 'meta' folder.\".format(len(meta_classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK-eqr9igq8n",
        "outputId": "c75a7fc2-49e5-40ab-a779-f02024efaa8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n",
            "\n",
            "101 food classes were found in the 'classes.txt' file in the 'meta' folder.\n"
          ]
        }
      ],
      "source": [
        "with open(meta_class_data, 'r') as f:\n",
        "    meta_classes = f.read().splitlines()  # Classes listed in the 'classes.txt' file in the 'meta' folder\n",
        "\n",
        "print(meta_classes)\n",
        "print(\"\")\n",
        "print(\"{0} food classes were found in the 'classes.txt' file in the 'meta' folder.\".format(len(meta_classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVJ2lZhYgruN"
      },
      "outputs": [],
      "source": [
        "# 'labels.txt'\n",
        "labels_data = os.path.join(metadata_path, 'labels.txt')\n",
        "\n",
        "# 'test.json'\n",
        "test_json_data = os.path.join(metadata_path, 'test.json')\n",
        "\n",
        "# 'test.txt'\n",
        "test_txt_data = os.path.join(metadata_path, 'test.txt')\n",
        "\n",
        "# 'train.json'\n",
        "train_json_data = os.path.join(metadata_path, 'train.json')\n",
        "\n",
        "# 'train.txt'\n",
        "train_txt_data = os.path.join(metadata_path, 'train.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ64IEongxCC",
        "outputId": "4672f3bd-5755-4fd3-f70e-55ce395cc80c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food_classes = ['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n",
        "len(food_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5rEGsVLhNBj"
      },
      "source": [
        "Let's pick 50 food classes randomly to train our model with:\n",
        "\n",
        "NOTE: If you try to do run the code below, you will get a different set of 50 subclasses of food as it is generated randomly. To reproduce the model results, refer to Section 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FteFaIIwhMPx"
      },
      "outputs": [],
      "source": [
        "# random_food_classes = random.sample(food_classes, 50)\n",
        "# random_food_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsxiD-nChXb3"
      },
      "source": [
        "Now, we need to get a path to each subclass in `random_food_classes`. To do this, we can use variable, `images_path` and append each subclass string-value to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Eq2I9RK9hYfc",
        "outputId": "544be137-668e-452a-c94e-90a044709c01"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/Othercomputers/My Computer/food-101/images/tacos'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# food_classes_path = []\n",
        "\n",
        "# for subclass in random_food_classes:\n",
        "#     sc_path = os.path.join(images_path, subclass)\n",
        "#     food_classes_path.append(sc_path)\n",
        "\n",
        "# food_classes_path[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmUU3z08hmKW"
      },
      "outputs": [],
      "source": [
        "# food_classes_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fLg5XfohsFy"
      },
      "source": [
        "Now, let's save our subclasses and paths conveniently in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augrI6XIhrk8"
      },
      "outputs": [],
      "source": [
        "# food_classes_dict = dict(zip(random_food_classes, food_classes_path))\n",
        "# food_classes_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFmZ_-miihy"
      },
      "source": [
        "Now, let's download these classes for safe-keeping and easy loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extaXWwKimjx"
      },
      "outputs": [],
      "source": [
        "# with open('food_classes_path.txt', 'w') as f:\n",
        "#     for item in food_classes_path:\n",
        "#         f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVzWem8iqFM"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# with open('food_classes_dict.txt', 'w') as f:\n",
        "#     f.write(json.dumps(food_classes_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td5L1zwNinEL"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "S_YYnR7Vi6ur",
        "outputId": "7b914a30-3479-4712-dda3-5592fac3bed3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0df8de7b-6ffd-42a7-a79e-f0f200ec3b29\", \"food_classes_path.txt\", 3467)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_bef2f98b-e798-4a9e-b295-bf52af8253e2\", \"food_classes_dict.txt\", 4334)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# files.download('food_classes_path.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xBWxkY_hi-sy",
        "outputId": "10f5cfee-279d-4d0b-d7e0-ec54cc6f2596"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_aad326b3-ebe7-40df-a256-987ad7a179b7\", \"food_classes_dict.txt\", 4334)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# files.download('food_classes_dict.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCW9joViBZH_"
      },
      "source": [
        "# Section 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cqVrzjZh5w-"
      },
      "source": [
        "Next, we need to get a list of each file from each subclass path in `subclasses_path` so that we can visualize some examples and do some basic statistics (if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU9TVyhWjShA"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/Large Scale Machine Learning 2/Final Project/food_classes_path.txt', 'r') as f:\n",
        "    food_classes_path = [line.strip() for line in f]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Large Scale Machine Learning 2/Final Project/food_classes_dict.txt', 'r') as f:\n",
        "    food_classes_dict = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WovxbUw6pIqr"
      },
      "outputs": [],
      "source": [
        "# with open('food_classes_path.txt', 'r') as f:\n",
        "#     food_classes_path = [line.strip() for line in f]\n",
        "\n",
        "# with open('food_classes_dict.txt', 'r') as f:\n",
        "#     food_classes_dict = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WfAVLslfkmxY",
        "outputId": "bb2201bf-4d82-448a-c85c-c44a4020db84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/Othercomputers/My Computer/food-101/images/carrot_cake'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food_classes_path[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pyNYpSPh5SV"
      },
      "outputs": [],
      "source": [
        "# Lists to store images conveniently\n",
        "tacos_imgs = []\n",
        "hamburger_imgs = []\n",
        "chocolate_cake_imgs = []\n",
        "bread_pudding_imgs = []\n",
        "creme_brulee_imgs = []\n",
        "fried_rice_imgs = []\n",
        "macarons_imgs = []\n",
        "bruschetta_imgs = []\n",
        "lobster_bisque_imgs = []\n",
        "garlic_bread_imgs = []\n",
        "fried_calamari_imgs = []\n",
        "deviled_eggs_imgs = []\n",
        "gyoza_imgs = []\n",
        "french_toast_imgs = []\n",
        "steak_imgs = []\n",
        "omelette_imgs = []\n",
        "pancakes_imgs = []\n",
        "chicken_wings_imgs = []\n",
        "samosa_imgs = []\n",
        "spaghetti_bolognese_imgs = []\n",
        "pizza_imgs = []\n",
        "fish_and_chips_imgs = []\n",
        "crab_cakes_imgs = []\n",
        "panna_cotta_imgs = []\n",
        "baby_back_ribs_imgs = []\n",
        "pork_chop_imgs = []\n",
        "paella_imgs = []\n",
        "bibimbap_imgs = []\n",
        "huevos_rancheros_imgs = []\n",
        "takoyaki_imgs = []\n",
        "seaweed_salad_imgs = []\n",
        "onion_rings_imgs = []\n",
        "hummus_imgs = []\n",
        "foie_gras_imgs = []\n",
        "risotto_imgs = []\n",
        "chicken_curry_imgs = []\n",
        "croque_madame_imgs = []\n",
        "falafel_imgs = []\n",
        "lobster_roll_sandwich_imgs = []\n",
        "peking_duck_imgs = []\n",
        "shrimp_and_grits_imgs = []\n",
        "donuts_imgs = []\n",
        "mussels_imgs = []\n",
        "edamame_imgs = []\n",
        "ceviche_imgs = []\n",
        "grilled_salmon_imgs = []\n",
        "hot_and_sour_soup_imgs = []\n",
        "nachos_imgs = []\n",
        "ramen_imgs = []\n",
        "carrot_cake_imgs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gygtbLgypIqs",
        "outputId": "128f8e4e-6e27-47db-e599-4bf889af2324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['tacos', 'hamburger', 'chocolate_cake', 'bread_pudding', 'creme_brulee', 'fried_rice', 'macarons', 'bruschetta', 'lobster_bisque', 'garlic_bread', 'fried_calamari', 'deviled_eggs', 'gyoza', 'french_toast', 'steak', 'omelette', 'pancakes', 'chicken_wings', 'samosa', 'spaghetti_bolognese', 'pizza', 'fish_and_chips', 'crab_cakes', 'panna_cotta', 'baby_back_ribs', 'pork_chop', 'paella', 'bibimbap', 'huevos_rancheros', 'takoyaki', 'seaweed_salad', 'onion_rings', 'hummus', 'foie_gras', 'risotto', 'chicken_curry', 'croque_madame', 'falafel', 'lobster_roll_sandwich', 'peking_duck', 'shrimp_and_grits', 'donuts', 'mussels', 'edamame', 'ceviche', 'grilled_salmon', 'hot_and_sour_soup', 'nachos', 'ramen', 'carrot_cake'])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food_classes_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ftyKRX_klpP"
      },
      "outputs": [],
      "source": [
        "# Tacos\n",
        "taco_imgs_dir = os.listdir(food_classes_dict['tacos'])\n",
        "\n",
        "for img in taco_imgs_dir:\n",
        "    tacos_imgs.append(os.path.join(food_classes_dict['tacos'], img))\n",
        "\n",
        "# Hamburger\n",
        "hamburger_imgs_dir = os.listdir(food_classes_dict['hamburger'])\n",
        "\n",
        "for img in hamburger_imgs_dir:\n",
        "    hamburger_imgs.append(os.path.join(food_classes_dict['hamburger'], img))\n",
        "\n",
        "# Chocolate Cake\n",
        "chocolate_cake_imgs_dir = os.listdir(food_classes_dict['chocolate_cake'])\n",
        "\n",
        "for img in chocolate_cake_imgs_dir:\n",
        "    chocolate_cake_imgs.append(os.path.join(food_classes_dict['chocolate_cake'], img))\n",
        "\n",
        "# Bread Pudding\n",
        "bread_pudding_imgs_dir = os.listdir(food_classes_dict['bread_pudding'])\n",
        "\n",
        "for img in bread_pudding_imgs_dir:\n",
        "    bread_pudding_imgs.append(os.path.join(food_classes_dict['bread_pudding'], img))\n",
        "\n",
        "# Creme Brulee\n",
        "creme_brulee_imgs_dir = os.listdir(food_classes_dict['creme_brulee'])\n",
        "\n",
        "for img in creme_brulee_imgs_dir:\n",
        "    creme_brulee_imgs.append(os.path.join(food_classes_dict['creme_brulee'], img))\n",
        "\n",
        "# Fried Rice\n",
        "fried_rice_imgs_dir = os.listdir(food_classes_dict['fried_rice'])\n",
        "\n",
        "for img in fried_rice_imgs_dir:\n",
        "    fried_rice_imgs.append(os.path.join(food_classes_dict['fried_rice'], img))\n",
        "\n",
        "# Macarons\n",
        "macarons_imgs_dir = os.listdir(food_classes_dict['macarons'])\n",
        "\n",
        "for img in macarons_imgs_dir:\n",
        "    macarons_imgs.append(os.path.join(food_classes_dict['macarons'], img))\n",
        "\n",
        "# Bruschetta\n",
        "bruschetta_imgs_dir = os.listdir(food_classes_dict['bruschetta'])\n",
        "\n",
        "for img in bruschetta_imgs_dir:\n",
        "    bruschetta_imgs.append(os.path.join(food_classes_dict['bruschetta'], img))\n",
        "\n",
        "# Lobster Bisque\n",
        "lobster_bisque_imgs_dir = os.listdir(food_classes_dict['lobster_bisque'])\n",
        "\n",
        "for img in lobster_bisque_imgs_dir:\n",
        "    lobster_bisque_imgs.append(os.path.join(food_classes_dict['lobster_bisque'], img))\n",
        "\n",
        "# Garlic Bread\n",
        "garlic_bread_imgs_dir = os.listdir(food_classes_dict['garlic_bread'])\n",
        "\n",
        "for img in garlic_bread_imgs_dir:\n",
        "    garlic_bread_imgs.append(os.path.join(food_classes_dict['garlic_bread'], img))\n",
        "\n",
        "# Fried Calamari\n",
        "fried_calamari_imgs_dir = os.listdir(food_classes_dict['fried_calamari'])\n",
        "\n",
        "for img in fried_calamari_imgs_dir:\n",
        "    fried_calamari_imgs.append(os.path.join(food_classes_dict['fried_calamari'], img))\n",
        "\n",
        "# Deviled Eggs\n",
        "deviled_eggs_imgs_dir = os.listdir(food_classes_dict['deviled_eggs'])\n",
        "\n",
        "for img in deviled_eggs_imgs_dir:\n",
        "    deviled_eggs_imgs.append(os.path.join(food_classes_dict['deviled_eggs'], img))\n",
        "\n",
        "# Gyoza\n",
        "gyoza_imgs_dir = os.listdir(food_classes_dict['gyoza'])\n",
        "\n",
        "for img in gyoza_imgs_dir:\n",
        "    gyoza_imgs.append(os.path.join(food_classes_dict['gyoza'], img))\n",
        "\n",
        "# French Toast\n",
        "french_toast_imgs_dir = os.listdir(food_classes_dict['french_toast'])\n",
        "\n",
        "for img in french_toast_imgs_dir:\n",
        "    french_toast_imgs.append(os.path.join(food_classes_dict['french_toast'], img))\n",
        "\n",
        "# Steak\n",
        "steak_imgs_dir = os.listdir(food_classes_dict['steak'])\n",
        "\n",
        "for img in steak_imgs_dir:\n",
        "    steak_imgs.append(os.path.join(food_classes_dict['steak'], img))\n",
        "\n",
        "# Omelette\n",
        "omelette_imgs_dir = os.listdir(food_classes_dict['omelette'])\n",
        "\n",
        "for img in omelette_imgs_dir:\n",
        "    omelette_imgs.append(os.path.join(food_classes_dict['omelette'], img))\n",
        "\n",
        "# Pancakes\n",
        "pancakes_imgs_dir = os.listdir(food_classes_dict['pancakes'])\n",
        "\n",
        "for img in pancakes_imgs_dir:\n",
        "    pancakes_imgs.append(os.path.join(food_classes_dict['pancakes'], img))\n",
        "\n",
        "# Chicken Wings\n",
        "chicken_wings_imgs_dir = os.listdir(food_classes_dict['chicken_wings'])\n",
        "\n",
        "for img in chicken_wings_imgs_dir:\n",
        "    chicken_wings_imgs.append(os.path.join(food_classes_dict['chicken_wings'], img))\n",
        "\n",
        "# Samosa\n",
        "samosa_imgs_dir = os.listdir(food_classes_dict['samosa'])\n",
        "\n",
        "for img in samosa_imgs_dir:\n",
        "    samosa_imgs.append(os.path.join(food_classes_dict['samosa'], img))\n",
        "\n",
        "# Spaghetti Bolognese\n",
        "spaghetti_bolognese_imgs_dir = os.listdir(food_classes_dict['spaghetti_bolognese'])\n",
        "\n",
        "for img in spaghetti_bolognese_imgs_dir:\n",
        "    spaghetti_bolognese_imgs.append(os.path.join(food_classes_dict['spaghetti_bolognese'], img))\n",
        "\n",
        "# Pizza\n",
        "pizza_imgs_dir = os.listdir(food_classes_dict['pizza'])\n",
        "\n",
        "for img in pizza_imgs_dir:\n",
        "    pizza_imgs.append(os.path.join(food_classes_dict['pizza'], img))\n",
        "\n",
        "# Fish and Chips\n",
        "fish_and_chips_imgs_dir = os.listdir(food_classes_dict['fish_and_chips'])\n",
        "\n",
        "for img in fish_and_chips_imgs_dir:\n",
        "    fish_and_chips_imgs.append(os.path.join(food_classes_dict['fish_and_chips'], img))\n",
        "\n",
        "# Crab Cakes\n",
        "crab_cakes_imgs_dir = os.listdir(food_classes_dict['crab_cakes'])\n",
        "\n",
        "for img in crab_cakes_imgs_dir:\n",
        "    crab_cakes_imgs.append(os.path.join(food_classes_dict['crab_cakes'], img))\n",
        "\n",
        "# Panna Cotta\n",
        "panna_cotta_imgs_dir = os.listdir(food_classes_dict['panna_cotta'])\n",
        "\n",
        "for img in panna_cotta_imgs_dir:\n",
        "    panna_cotta_imgs.append(os.path.join(food_classes_dict['panna_cotta'], img))\n",
        "\n",
        "# Baby Back Ribs\n",
        "baby_back_ribs_imgs_dir = os.listdir(food_classes_dict['baby_back_ribs'])\n",
        "\n",
        "for img in baby_back_ribs_imgs_dir:\n",
        "    baby_back_ribs_imgs.append(os.path.join(food_classes_dict['baby_back_ribs'], img))\n",
        "\n",
        "# Pork Chop\n",
        "pork_chop_imgs_dir = os.listdir(food_classes_dict['pork_chop'])\n",
        "\n",
        "for img in pork_chop_imgs_dir:\n",
        "    pork_chop_imgs.append(os.path.join(food_classes_dict['pork_chop'], img))\n",
        "\n",
        "# Paella\n",
        "paella_imgs_dir = os.listdir(food_classes_dict['paella'])\n",
        "\n",
        "for img in paella_imgs_dir:\n",
        "    paella_imgs.append(os.path.join(food_classes_dict['paella'], img))\n",
        "\n",
        "# Bibimbap\n",
        "bibimbap_imgs_dir = os.listdir(food_classes_dict['bibimbap'])\n",
        "\n",
        "for img in bibimbap_imgs_dir:\n",
        "    bibimbap_imgs.append(os.path.join(food_classes_dict['bibimbap'], img))\n",
        "\n",
        "# Huevos Rancheros\n",
        "huevos_rancheros_imgs_dir = os.listdir(food_classes_dict['huevos_rancheros'])\n",
        "\n",
        "for img in huevos_rancheros_imgs_dir:\n",
        "    huevos_rancheros_imgs.append(os.path.join(food_classes_dict['huevos_rancheros'], img))\n",
        "\n",
        "# Takoyaki\n",
        "takoyaki_imgs_dir = os.listdir(food_classes_dict['takoyaki'])\n",
        "\n",
        "for img in takoyaki_imgs_dir:\n",
        "    takoyaki_imgs.append(os.path.join(food_classes_dict['takoyaki'], img))\n",
        "\n",
        "# Seaweed Salad\n",
        "seaweed_salad_imgs_dir = os.listdir(food_classes_dict['seaweed_salad'])\n",
        "\n",
        "for img in seaweed_salad_imgs_dir:\n",
        "    seaweed_salad_imgs.append(os.path.join(food_classes_dict['seaweed_salad'], img))\n",
        "\n",
        "# Onion Rings\n",
        "onion_rings_imgs_dir = os.listdir(food_classes_dict['onion_rings'])\n",
        "\n",
        "for img in onion_rings_imgs_dir:\n",
        "    onion_rings_imgs.append(os.path.join(food_classes_dict['onion_rings'], img))\n",
        "\n",
        "# Hummus\n",
        "hummus_imgs_dir = os.listdir(food_classes_dict['hummus'])\n",
        "\n",
        "for img in hummus_imgs_dir:\n",
        "    hummus_imgs.append(os.path.join(food_classes_dict['hummus'], img))\n",
        "\n",
        "# Foie Gras\n",
        "foie_gras_imgs_dir = os.listdir(food_classes_dict['foie_gras'])\n",
        "\n",
        "for img in foie_gras_imgs_dir:\n",
        "    foie_gras_imgs.append(os.path.join(food_classes_dict['foie_gras'], img))\n",
        "\n",
        "# Risotto\n",
        "risotto_imgs_dir = os.listdir(food_classes_dict['risotto'])\n",
        "\n",
        "for img in risotto_imgs_dir:\n",
        "    risotto_imgs.append(os.path.join(food_classes_dict['risotto'], img))\n",
        "\n",
        "# Chicken Curry\n",
        "chicken_curry_imgs_dir = os.listdir(food_classes_dict['chicken_curry'])\n",
        "\n",
        "for img in chicken_curry_imgs_dir:\n",
        "    chicken_curry_imgs.append(os.path.join(food_classes_dict['chicken_curry'], img))\n",
        "\n",
        "# Croque Madame\n",
        "croque_madame_imgs_dir = os.listdir(food_classes_dict['croque_madame'])\n",
        "\n",
        "for img in croque_madame_imgs_dir:\n",
        "    croque_madame_imgs.append(os.path.join(food_classes_dict['croque_madame'], img))\n",
        "\n",
        "# Falafel\n",
        "falafel_imgs_dir = os.listdir(food_classes_dict['falafel'])\n",
        "\n",
        "for img in falafel_imgs_dir:\n",
        "    falafel_imgs.append(os.path.join(food_classes_dict['falafel'], img))\n",
        "\n",
        "# Lobster Roll Sandwich\n",
        "lobster_roll_sandwich_imgs_dir = os.listdir(food_classes_dict['lobster_roll_sandwich'])\n",
        "\n",
        "for img in lobster_roll_sandwich_imgs_dir:\n",
        "    lobster_roll_sandwich_imgs.append(os.path.join(food_classes_dict['lobster_roll_sandwich'], img))\n",
        "\n",
        "# Peking Duck\n",
        "peking_duck_imgs_dir = os.listdir(food_classes_dict['peking_duck'])\n",
        "\n",
        "for img in peking_duck_imgs_dir:\n",
        "    peking_duck_imgs.append(os.path.join(food_classes_dict['peking_duck'], img))\n",
        "\n",
        "# Shrimp and Grits\n",
        "shrimp_and_grits_imgs_dir = os.listdir(food_classes_dict['shrimp_and_grits'])\n",
        "\n",
        "for img in shrimp_and_grits_imgs_dir:\n",
        "    shrimp_and_grits_imgs.append(os.path.join(food_classes_dict['shrimp_and_grits'], img))\n",
        "\n",
        "# Donuts\n",
        "donuts_imgs_dir = os.listdir(food_classes_dict['donuts'])\n",
        "\n",
        "for img in donuts_imgs_dir:\n",
        "    donuts_imgs.append(os.path.join(food_classes_dict['donuts'], img))\n",
        "\n",
        "# Mussels\n",
        "mussels_imgs_dir = os.listdir(food_classes_dict['mussels'])\n",
        "\n",
        "for img in mussels_imgs_dir:\n",
        "    mussels_imgs.append(os.path.join(food_classes_dict['mussels'], img))\n",
        "\n",
        "# Edamame\n",
        "edamame_imgs_dir = os.listdir(food_classes_dict['edamame'])\n",
        "\n",
        "for img in edamame_imgs_dir:\n",
        "    edamame_imgs.append(os.path.join(food_classes_dict['edamame'], img))\n",
        "\n",
        "# Ceviche\n",
        "ceviche_imgs_dir = os.listdir(food_classes_dict['ceviche'])\n",
        "\n",
        "for img in ceviche_imgs_dir:\n",
        "    ceviche_imgs.append(os.path.join(food_classes_dict['ceviche'], img))\n",
        "\n",
        "# Grilled Salad\n",
        "grilled_salmon_imgs_dir = os.listdir(food_classes_dict['grilled_salmon'])\n",
        "\n",
        "for img in grilled_salmon_imgs_dir:\n",
        "    grilled_salmon_imgs.append(os.path.join(food_classes_dict['grilled_salmon'], img))\n",
        "\n",
        "# Hot and Sour Soup\n",
        "hot_and_sour_soup_imgs_dir = os.listdir(food_classes_dict['hot_and_sour_soup'])\n",
        "\n",
        "for img in hot_and_sour_soup_imgs_dir:\n",
        "    hot_and_sour_soup_imgs.append(os.path.join(food_classes_dict['hot_and_sour_soup'], img))\n",
        "\n",
        "# Nachos\n",
        "nachos_imgs_dir = os.listdir(food_classes_dict['nachos'])\n",
        "\n",
        "for img in nachos_imgs_dir:\n",
        "    nachos_imgs.append(os.path.join(food_classes_dict['nachos'], img))\n",
        "\n",
        "# Ramen\n",
        "ramen_imgs_dir = os.listdir(food_classes_dict['ramen'])\n",
        "\n",
        "for img in ramen_imgs_dir:\n",
        "    ramen_imgs.append(os.path.join(food_classes_dict['ramen'], img))\n",
        "\n",
        "# Carrot Cake\n",
        "carrot_cake_imgs_dir = os.listdir(food_classes_dict['carrot_cake'])\n",
        "\n",
        "for img in carrot_cake_imgs_dir:\n",
        "    carrot_cake_imgs.append(os.path.join(food_classes_dict['carrot_cake'], img))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhKqqAqtpIqt"
      },
      "outputs": [],
      "source": [
        "all_subclass_imgs = {\n",
        "    'tacos': tacos_imgs,\n",
        "    'hamburger': hamburger_imgs,\n",
        "    'chocolate_cake': chocolate_cake_imgs,\n",
        "    'bread_pudding': bread_pudding_imgs,\n",
        "    'creme_brulee': creme_brulee_imgs,\n",
        "    'fried_rice': fried_rice_imgs,\n",
        "    'macarons': macarons_imgs,\n",
        "    'bruschetta': bruschetta_imgs,\n",
        "    'lobster_bisque': lobster_bisque_imgs,\n",
        "    'garlic_bread': garlic_bread_imgs,\n",
        "    'fried_calamari': fried_calamari_imgs,\n",
        "    'deviled_eggs': deviled_eggs_imgs,\n",
        "    'gyoza': gyoza_imgs,\n",
        "    'french_toast': french_toast_imgs,\n",
        "    'steak': steak_imgs,\n",
        "    'omelette': omelette_imgs,\n",
        "    'pancakes': pancakes_imgs,\n",
        "    'chicken_wings': chicken_wings_imgs,\n",
        "    'samosa': samosa_imgs,\n",
        "    'spaghetti_bolognese': spaghetti_bolognese_imgs,\n",
        "    'pizza': pizza_imgs,\n",
        "    'fish_and_chips': fish_and_chips_imgs,\n",
        "    'crab_cakes': crab_cakes_imgs,\n",
        "    'panna_cotta': panna_cotta_imgs,\n",
        "    'baby_back_ribs': baby_back_ribs_imgs,\n",
        "    'pork_chop': pork_chop_imgs,\n",
        "    'paella': paella_imgs,\n",
        "    'bibimbap': bibimbap_imgs,\n",
        "    'huevos_rancheros': huevos_rancheros_imgs,\n",
        "    'takoyaki': takoyaki_imgs,\n",
        "    'seaweed_salad': seaweed_salad_imgs,\n",
        "    'onion_rings': onion_rings_imgs,\n",
        "    'hummus': hummus_imgs,\n",
        "    'foie_gras': foie_gras_imgs,\n",
        "    'risotto': risotto_imgs,\n",
        "    'chicken_curry': chicken_curry_imgs,\n",
        "    'croque_madame': croque_madame_imgs,\n",
        "    'falafel': falafel_imgs,\n",
        "    'lobster_roll_sandwich': lobster_roll_sandwich_imgs,\n",
        "    'peking_duck': peking_duck_imgs,\n",
        "    'shrimp_and_grits': shrimp_and_grits_imgs,\n",
        "    'donuts': donuts_imgs,\n",
        "    'mussels': mussels_imgs,\n",
        "    'edamame': edamame_imgs,\n",
        "    'ceviche': ceviche_imgs,\n",
        "    'grilled_salmon': grilled_salmon_imgs,\n",
        "    'hot_and_sour_soup': hot_and_sour_soup_imgs,\n",
        "    'nachos': nachos_imgs,\n",
        "    'ramen': ramen_imgs,\n",
        "    'carrot_cake': carrot_cake_imgs\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "An8ysGnXraS0",
        "outputId": "e7d8d34c-73fc-4738-c215-420929d5163f"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_703a217f-5288-4a2a-a7f2-3ee9ba479d6a\", \"all_subclass_imgs.txt\", 4201192)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# with open('all_subclass_imgs.txt', 'w') as f:\n",
        "#     f.write(json.dumps(all_subclass_imgs))\n",
        "\n",
        "# files.download('all_subclass_imgs.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pOpJx_gpIqt"
      },
      "outputs": [],
      "source": [
        "for key, value in all_subclass_imgs.items():\n",
        "    print(\"{0} has {1} images.\".format(key, len(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF67kOrZBZIB"
      },
      "source": [
        "# Section 3: Preprocessing for ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5JIOmzSsKKT"
      },
      "source": [
        "## 3.1 Split the data from train into `current_train` and `current_val` (`val_size = 10%`). `random_state` is fixed for reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yo0mW6CsMXb"
      },
      "source": [
        "Before proceeding forward and splitting the data, we need to use only the subset of the data containing the subclasses we are going to evaluate. We also need to do the same for the test dataset too.\n",
        "\n",
        "Let's do this below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj11ANCGskuV"
      },
      "outputs": [],
      "source": [
        "subclasses = ['tacos',\n",
        "              'hamburger',\n",
        "              'chocolate_cake',\n",
        "              'bread_pudding',\n",
        "              'creme_brulee',\n",
        "              'fried_rice',\n",
        "              'macarons',\n",
        "              'bruschetta',\n",
        "              'lobster_bisque',\n",
        "              'garlic_bread',\n",
        "              'fried_calamari',\n",
        "              'deviled_eggs',\n",
        "              'gyoza',\n",
        "              'french_toast',\n",
        "              'steak',\n",
        "              'omelette',\n",
        "              'pancakes',\n",
        "              'chicken_wings',\n",
        "              'samosa',\n",
        "              'spaghetti_bolognese',\n",
        "              'pizza',\n",
        "              'fish_and_chips',\n",
        "              'crab_cakes',\n",
        "              'panna_cotta',\n",
        "              'baby_back_ribs',\n",
        "              'pork_chop',\n",
        "              'paella',\n",
        "              'bibimbap',\n",
        "              'huevos_rancheros',\n",
        "              'takoyaki',\n",
        "              'seaweed_salad',\n",
        "              'onion_rings',\n",
        "              'hummus',\n",
        "              'foie_gras',\n",
        "              'risotto',\n",
        "              'chicken_curry',\n",
        "              'croque_madame',\n",
        "              'falafel',\n",
        "              'lobster_roll_sandwich',\n",
        "              'peking_duck',\n",
        "              'shrimp_and_grits',\n",
        "              'donuts',\n",
        "              'mussels',\n",
        "              'edamame',\n",
        "              'ceviche',\n",
        "              'grilled_salmon',\n",
        "              'hot_and_sour_soup',\n",
        "              'nachos',\n",
        "              'ramen',\n",
        "              'carrot_cake']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtI-ICpMQbVT"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary where the keys are the indices and the values are the food subclasses\n",
        "food_class_labels = {i: subclass for i, subclass in enumerate(subclasses)}\n",
        "\n",
        "# Write the dictionary to a JSON file\n",
        "with open('subclasses.json', 'w') as f:\n",
        "    json.dump(food_class_labels, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--PlJUqhsPAd",
        "outputId": "956716ff-5770-412f-f78c-2944e859a062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before filtering, there were 75750 observations in the training data set.\n",
            "After filtering, there are 37500 observations in the training data set.\n"
          ]
        }
      ],
      "source": [
        "# Training data preprocessing\n",
        "train = pd.read_csv(train_txt_data, sep=\" \", header=None, usecols=[0])\n",
        "filtered_train_data = [x for x in train[0] if any(subclass in x for subclass in subclasses)]\n",
        "\n",
        "print(\"Before filtering, there were {0} observations in the training data set.\".format(len(train)))\n",
        "print(\"After filtering, there are {0} observations in the training data set.\".format(len(filtered_train_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiOzHqxTs-9C",
        "outputId": "d39659fd-df75-43d5-c261-abd45af68562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before filtering, there were 25250 observations in the training data set.\n",
            "After filtering, there are 12500 observations in the training data set.\n"
          ]
        }
      ],
      "source": [
        "# Test data preprocessing\n",
        "test = pd.read_csv(test_txt_data, sep=\" \", header=None, usecols=[0])\n",
        "filtered_test_data = [x for x in test[0] if any(subclass in x for subclass in subclasses)]\n",
        "\n",
        "print(\"Before filtering, there were {0} observations in the training data set.\".format(len(test)))\n",
        "print(\"After filtering, there are {0} observations in the training data set.\".format(len(filtered_test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc3j7SsHtFPQ"
      },
      "source": [
        "Great. Now using `filtered_train_data`, we can create the `current_train` and `current_val` variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXBuhddWtEmS",
        "outputId": "efc9f206-2f2a-4471-d536-a0db67b3d577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Of the total 37500 observations in filtered_train_data, there are 33750 observations in current_train and 3750 observations in current_val.\n"
          ]
        }
      ],
      "source": [
        "current_train, current_val = train_test_split(filtered_train_data, test_size=0.1, random_state=42)\n",
        "print(\"Of the total {0} observations in filtered_train_data, there are {1} observations in current_train and {2} observations in current_val.\".format(len(filtered_train_data), len(current_train), len(current_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXfzyj8stDFf",
        "outputId": "de2c4ddb-e64a-4245-b220-edee112a1cd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['panna_cotta/3697933',\n",
              " 'grilled_salmon/937466',\n",
              " 'carrot_cake/3855103',\n",
              " 'baby_back_ribs/2511963',\n",
              " 'hamburger/3759470']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_train[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE_XJ3EQtM1u",
        "outputId": "a4798090-49d0-4d23-960b-59aa9c24f2ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['omelette/210179',\n",
              " 'hot_and_sour_soup/274720',\n",
              " 'carrot_cake/1436835',\n",
              " 'nachos/3517672',\n",
              " 'samosa/1982708']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_val[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jJoRCkfBZIG"
      },
      "source": [
        "# Section 4: Creating Training Directories for Data\n",
        "\n",
        "Please note: The code in Section 4 only needs to be run once only. Do not run it more than once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEU6g9TQu9z2"
      },
      "source": [
        "## `current_train` folder + data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwL08-3Eu-Vg"
      },
      "outputs": [],
      "source": [
        "# Please update these paths accordingly for your local machine.\n",
        "images_path = '/content/drive/Othercomputers/My Computer/food-101/images'  # Path to images folder within project directory\n",
        "project_path = '/content/drive/Othercomputers/My Computer/food-101'  # Path to project directory, food-101 folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzobFSkuvH4H"
      },
      "outputs": [],
      "source": [
        "current_train_path = os.path.join(project_path, 'current_train')\n",
        "os.makedirs(current_train_path, exist_ok=True)\n",
        "\n",
        "# Create subfolders within current_train_path for each subclass of subclasses variable\n",
        "\n",
        "for subclass in subclasses:\n",
        "    current_train_subclass_path = os.path.join(current_train_path, subclass)\n",
        "    os.makedirs(current_train_subclass_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVYJhSolvN2g"
      },
      "outputs": [],
      "source": [
        "# First, let's create a dictionary of values\n",
        "\n",
        "current_train_dict = {}\n",
        "\n",
        "for img in current_train:\n",
        "    subclass_name = img.split('/')[0]\n",
        "    if subclass_name not in current_train_dict:\n",
        "        current_train_dict[subclass_name] = []\n",
        "    current_train_dict[subclass_name].append(img)\n",
        "\n",
        "for key, value in current_train_dict.items():\n",
        "    for img in value:\n",
        "        img_path = os.path.join(images_path, img + '.jpg')\n",
        "        shutil.copy(img_path, os.path.join(current_train_path, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgI0SWuUvmMm"
      },
      "source": [
        "## `current_val` folder + data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkHsmNFvfOr"
      },
      "outputs": [],
      "source": [
        "current_val_path = os.path.join(project_path, 'current_val')\n",
        "os.makedirs(current_val_path, exist_ok=True)\n",
        "\n",
        "# Create subfolders within current_train_path for each subclass of subclasses variable\n",
        "\n",
        "for subclass in subclasses:\n",
        "    current_val_subclass_path = os.path.join(current_val_path, subclass)\n",
        "    os.makedirs(current_val_subclass_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k1x7TVGvnr3"
      },
      "outputs": [],
      "source": [
        "# First, let's create a dictionary of values\n",
        "\n",
        "current_val_dict = {}\n",
        "\n",
        "for img in current_val:\n",
        "    subclass_name = img.split('/')[0]\n",
        "    if subclass_name not in current_val_dict:\n",
        "        current_val_dict[subclass_name] = []\n",
        "    current_val_dict[subclass_name].append(img)\n",
        "\n",
        "for key, value in current_val_dict.items():\n",
        "    for img in value:\n",
        "        img_path = os.path.join(images_path, img + '.jpg')\n",
        "        shutil.copy(img_path, os.path.join(current_val_path, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB9ISc7_vyfu"
      },
      "source": [
        "## `filtered_test_data` folder + data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0bDZPM-v1SG"
      },
      "outputs": [],
      "source": [
        "filtered_test_data_path = os.path.join(project_path, 'filtered_test_data')\n",
        "os.makedirs(filtered_test_data_path, exist_ok=True)\n",
        "\n",
        "# Create subfolders within current_train_path for each subclass of subclasses variable\n",
        "\n",
        "for subclass in subclasses:\n",
        "    current_test_subclass_path = os.path.join(filtered_test_data_path, subclass)\n",
        "    os.makedirs(current_test_subclass_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMz42ZlPv1Pu"
      },
      "outputs": [],
      "source": [
        "# First, let's create a dictionary of values\n",
        "current_test_data_dict = {}\n",
        "\n",
        "for img in filtered_test_data:\n",
        "    subclass_name = img.split('/')[0]\n",
        "    if subclass_name not in current_test_data_dict:\n",
        "        current_test_data_dict[subclass_name] = []\n",
        "    current_test_data_dict[subclass_name].append(img)\n",
        "\n",
        "for key, value in current_test_data_dict.items():\n",
        "    for img in value:\n",
        "        img_path = os.path.join(images_path, img + '.jpg')\n",
        "        shutil.copy(img_path, os.path.join(filtered_test_data_path, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNNSc0M4BZIH"
      },
      "source": [
        "# Section 5: ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFlNKJODM4O2"
      },
      "outputs": [],
      "source": [
        "current_train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=means,std=stdevs)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN1ix27uMbxX"
      },
      "outputs": [],
      "source": [
        "current_train_path = '/content/drive/Othercomputers/My Computer/food-101/current_train'  # Path to current_train folder that is contained within the Food-101 folder\n",
        "\n",
        "current_train_temp = datasets.ImageFolder(root = current_train_path, transform=current_train_transform)\n",
        "\n",
        "current_train_dataloader = torch.utils.data.DataLoader(current_train_temp, batch_size = 128, shuffle = True, pin_memory = True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbgVEQhOX7q"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(current_train_dataloader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe75djRdtT5C"
      },
      "source": [
        "## 5.1 ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L7NNBsFtW-j",
        "outputId": "4637e768-af6c-4b91-8ccd-6730730c66e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 89.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "print(resnet50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzuNWtWXtgr5"
      },
      "source": [
        "### Fine-tuning\n",
        "\n",
        "Since we are only working with a subset of 50 classes, we can replace the Fully-Connected layer in the output layer with a new Fully-Connected layer containing 50 neurons only, one neuron for each of the subclasses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2axPdGltbMp",
        "outputId": "20a71321-ea3b-46d0-e0fb-3fa6632a9313"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0170, -0.0124,  0.0219,  ...,  0.0018, -0.0076,  0.0001],\n",
              "        [-0.0320,  0.0167,  0.0280,  ..., -0.0133,  0.0163, -0.0001],\n",
              "        [ 0.0219,  0.0554, -0.0112,  ..., -0.0083,  0.0165, -0.0193],\n",
              "        ...,\n",
              "        [ 0.0675,  0.0302,  0.0134,  ...,  0.0272, -0.0175, -0.0418],\n",
              "        [ 0.0203,  0.0322,  0.0031,  ...,  0.0090,  0.0146,  0.0179],\n",
              "        [-0.0180,  0.0010, -0.0246,  ..., -0.0181,  0.0038,  0.0392]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Glorot initialization (https://pytorch.org/docs/stable/nn.init.html)\n",
        "resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 50)\n",
        "torch.nn.init.xavier_normal_(resnet50.fc.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYy-b-9ofuSJ"
      },
      "source": [
        "The next cell can take quite a while to run so I have hardcoded the means and stdevs as well for convenience but you are welcome to rerun the cell below if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pMltneztn9F"
      },
      "outputs": [],
      "source": [
        "means = []\n",
        "stdevs = []\n",
        "total_batches = len(current_train_dataloader)\n",
        "\n",
        "for i, (X, _) in enumerate(current_train_dataloader):\n",
        "    means.append(X.mean(dim = (0, 2, 3)))\n",
        "    stdevs.append(X.std(dim = (0, 2, 3)))\n",
        "    print(f\"Processed batch {i+1}/{total_batches}\")\n",
        "\n",
        "mean_ = torch.stack(means).mean(dim = 0)\n",
        "stdev_ = torch.stack(stdevs).mean(dim = 0)\n",
        "\n",
        "print(\"Means:\", mean_)\n",
        "print(\"Standard Deviations:\", stdev_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWutZxxdf3sB"
      },
      "outputs": [],
      "source": [
        "means = torch.tensor([0.5447, 0.4402, 0.3355])\n",
        "stdevs = torch.tensor([0.2713, 0.2733, 0.2785])\n",
        "\n",
        "# Means: tensor([0.5447, 0.4402, 0.3355])\n",
        "# Standard Deviations: tensor([0.2713, 0.2733, 0.2785])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jY_suIctyEh"
      },
      "source": [
        "Let's update `current_train_transform` and normalize it with the `means` and `stdevs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l0cKpxOgR4c"
      },
      "outputs": [],
      "source": [
        "current_train_path = '/content/drive/Othercomputers/My Computer/food-101/current_train'  # Path to current_train folder that is contained within the Food-101 folder\n",
        "\n",
        "current_train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=means,std=stdevs)\n",
        "])\n",
        "\n",
        "current_train_dataset = datasets.ImageFolder(root = current_train_path, transform = current_train_transform)\n",
        "\n",
        "current_train_dataloader = torch.utils.data.DataLoader(current_train_dataset, batch_size = 128, shuffle = True, pin_memory = True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-xgLDot3Zf"
      },
      "source": [
        "Let's also create a `torchvision.transforms` object for `current_val`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT_SUhvkgYSb"
      },
      "outputs": [],
      "source": [
        "current_val_path = '/content/drive/Othercomputers/My Computer/food-101/current_val'  # Path to current_val folder that is contained within the Food-101 folder\n",
        "\n",
        "current_val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=means,std=stdevs)\n",
        "])\n",
        "\n",
        "current_val_dataset = datasets.ImageFolder(root = current_val_path, transform = current_val_transform)\n",
        "\n",
        "current_val_dataloader = torch.utils.data.DataLoader(current_val_dataset, batch_size = 128, shuffle = True, pin_memory = True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDD1jM0dgpT7"
      },
      "source": [
        "Let's also create a `torchvision.transforms` object for `filtered_test_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP0ttQqPgprZ"
      },
      "outputs": [],
      "source": [
        "filtered_test_data_path = '/content/drive/Othercomputers/My Computer/food-101/filtered_test_data'  # Path to filtered_test_data folder that is contained within the Food-101 folder\n",
        "\n",
        "test_data_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root = filtered_test_data_path, transform = test_data_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 128, shuffle = True, pin_memory = True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwnCdeyJg6Vb"
      },
      "source": [
        "# Section 6: Training ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2aE6K2zg_Jk",
        "outputId": "5470bd02-6502-4b91-d681-9fe4da79f7ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Nov  9 19:20:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_81RTY1hBnn",
        "outputId": "fdbfd7b1-ea5a-46de-df43-b44b4a70de34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 gpus available to use.\n"
          ]
        }
      ],
      "source": [
        "n_GPUs = torch.cuda.device_count()\n",
        "print(\"There are {0} gpus available to use.\".format(n_GPUs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-88euDnhAxK"
      },
      "outputs": [],
      "source": [
        "# With Fine-Tuning\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr = 1e-5\n",
        "weight_decay = 5e-4\n",
        "epochs = 14\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "params = [param for name, param in resnet50.named_parameters() if 'fc' not in str(name)]\n",
        "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "model_checkpoint = 1  # Save data every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p34z_im8hJRP"
      },
      "outputs": [],
      "source": [
        "resnet50.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3nvkqh3GDdu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE1HUMTahM_2",
        "outputId": "57955795-e94f-40e0-830e-f9716d8dec9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 14 epochs on 1 GPU(s).\n",
            "Epoch 1 / 14\n",
            "Training loss: 2.0432629585266113\n",
            "Training accuracy: 0.5347259044647217\n",
            "Validation loss: 1.52080237865448\n",
            "Validation accuracy: 0.6410666704177856\n",
            "\n",
            "Total training time was 367.214152097702 seconds.\n",
            "\n",
            "Epoch 2 / 14\n",
            "Training loss: 1.229315161705017\n",
            "Training accuracy: 0.704444408416748\n",
            "Validation loss: 1.1371982097625732\n",
            "Validation accuracy: 0.7237333655357361\n",
            "\n",
            "Total training time was 733.8126406669617 seconds.\n",
            "\n",
            "Epoch 3 / 14\n",
            "Training loss: 0.8809308409690857\n",
            "Training accuracy: 0.7841481566429138\n",
            "Validation loss: 0.9650610685348511\n",
            "Validation accuracy: 0.7567999958992004\n",
            "\n",
            "Total training time was 1100.878668308258 seconds.\n",
            "\n",
            "Epoch 4 / 14\n",
            "Training loss: 0.6587837338447571\n",
            "Training accuracy: 0.8402962684631348\n",
            "Validation loss: 0.8809050917625427\n",
            "Validation accuracy: 0.7781333327293396\n",
            "\n",
            "Total training time was 1465.950237751007 seconds.\n",
            "\n",
            "Epoch 5 / 14\n",
            "Training loss: 0.48785531520843506\n",
            "Training accuracy: 0.8866666555404663\n",
            "Validation loss: 0.8360317349433899\n",
            "Validation accuracy: 0.7885333299636841\n",
            "\n",
            "Total training time was 1830.9918558597565 seconds.\n",
            "\n",
            "Epoch 6 / 14\n",
            "Training loss: 0.351589173078537\n",
            "Training accuracy: 0.9258962869644165\n",
            "Validation loss: 0.8094191551208496\n",
            "Validation accuracy: 0.7864000201225281\n",
            "\n",
            "Total training time was 2197.717853784561 seconds.\n",
            "\n",
            "Epoch 7 / 14\n",
            "Training loss: 0.24591365456581116\n",
            "Training accuracy: 0.9553777575492859\n",
            "Validation loss: 0.7985988855361938\n",
            "Validation accuracy: 0.7944000363349915\n",
            "\n",
            "Total training time was 2563.575574874878 seconds.\n",
            "\n",
            "Epoch 8 / 14\n",
            "Training loss: 0.16580402851104736\n",
            "Training accuracy: 0.9756444096565247\n",
            "Validation loss: 0.8025572896003723\n",
            "Validation accuracy: 0.7919999957084656\n",
            "\n",
            "Total training time was 2929.609655380249 seconds.\n",
            "\n",
            "Epoch 9 / 14\n",
            "Training loss: 0.11190182715654373\n",
            "Training accuracy: 0.9866370558738708\n",
            "Validation loss: 0.8013766407966614\n",
            "Validation accuracy: 0.7898666858673096\n",
            "\n",
            "Total training time was 3296.807038784027 seconds.\n",
            "\n",
            "Epoch 10 / 14\n",
            "Training loss: 0.07555746287107468\n",
            "Training accuracy: 0.9924740791320801\n",
            "Validation loss: 0.7989142537117004\n",
            "Validation accuracy: 0.79093337059021\n",
            "\n",
            "Total training time was 3661.410085916519 seconds.\n",
            "\n",
            "Epoch 11 / 14\n",
            "Training loss: 0.05104130879044533\n",
            "Training accuracy: 0.99579256772995\n",
            "Validation loss: 0.8070665001869202\n",
            "Validation accuracy: 0.7917333245277405\n",
            "\n",
            "Total training time was 4029.4044234752655 seconds.\n",
            "\n",
            "Epoch 12 / 14\n",
            "Training loss: 0.035374484956264496\n",
            "Training accuracy: 0.9976296424865723\n",
            "Validation loss: 0.8069051504135132\n",
            "Validation accuracy: 0.7968000173568726\n",
            "\n",
            "Total training time was 4394.915893793106 seconds.\n",
            "\n",
            "Epoch 13 / 14\n",
            "Training loss: 0.024688592180609703\n",
            "Training accuracy: 0.9986962676048279\n",
            "Validation loss: 0.8245452642440796\n",
            "Validation accuracy: 0.793066680431366\n",
            "\n",
            "Total training time was 4761.014368772507 seconds.\n",
            "\n",
            "Epoch 14 / 14\n",
            "Training loss: 0.01776007004082203\n",
            "Training accuracy: 0.9994962811470032\n",
            "Validation loss: 0.8134047985076904\n",
            "Validation accuracy: 0.7970666885375977\n",
            "\n",
            "Total training time was 5127.63600897789 seconds.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "print(\"Training for {0} epochs on {1} GPU(s).\".format(epochs, n_GPUs))\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
        "\n",
        "    resnet50.train()\n",
        "\n",
        "    train_loss = torch.tensor(0., device = device)\n",
        "    train_accuracy = torch.tensor(0., device = device)\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(current_train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)  # y\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = resnet50(data)  # preds\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            train_loss += loss * current_train_dataloader.batch_size\n",
        "            train_accuracy += (output.argmax(dim=1) == target).float().sum()\n",
        "\n",
        "    if current_val_dataloader is not None:\n",
        "        resnet50.eval()\n",
        "\n",
        "        valid_loss = torch.tensor(0., device = device)\n",
        "        valid_accuracy = torch.tensor(0., device = device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(current_val_dataloader):\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)  # y\n",
        "                output = resnet50(data)  # preds\n",
        "                loss = criterion(output, target)\n",
        "                valid_loss += loss * current_val_dataloader.batch_size\n",
        "                valid_accuracy += (output.argmax(dim=1) == target).float().sum()\n",
        "\n",
        "    print(\"Training loss: {0}\".format(train_loss / len(current_train_dataloader.dataset)))\n",
        "    print(\"Training accuracy: {0}\".format(train_accuracy / len(current_train_dataloader.dataset)))\n",
        "\n",
        "    if current_val_dataloader is not None:\n",
        "        print(\"Validation loss: {0}\".format(valid_loss / len(current_val_dataloader.dataset)))\n",
        "        print(\"Validation accuracy: {0}\".format(valid_accuracy / len(current_val_dataloader.dataset)))\n",
        "\n",
        "    if epoch % model_checkpoint == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'resnet50_state_dict': resnet50.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }\n",
        "        torch.save(checkpoint, './checkpoint.pth.tar')\n",
        "\n",
        "        print()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(\"Total training time was {0} seconds.\".format(end - start))\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBzwxV6iti1R"
      },
      "source": [
        "## Evaluating model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvmt5ITotmiK",
        "outputId": "af71c8e5-d584-4115-c4d9-40d966293ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.57008\n",
            "Precision: 0.6547480922121868\n",
            "Recall: 0.57008\n",
            "F1 Score: 0.5742206075240507\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "resnet50.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)  # y_true values\n",
        "        output = resnet50(data)  # y_pred values\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "precision = metrics.precision_score(y_true, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Test accuracy: {0}\".format(accuracy))\n",
        "print(\"Precision: {0}\".format(precision))\n",
        "print(\"Recall: {0}\".format(recall))\n",
        "print(\"F1 Score: {0}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbkh2ZuSzUqa"
      },
      "source": [
        "# Save model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3pgK7h_zYUf"
      },
      "outputs": [],
      "source": [
        "resnet50_checkpoint = torch.load('/content/checkpoint.pth.tar', map_location=torch.device('cpu'))\n",
        "\n",
        "project_path = '/content/drive/MyDrive/Colab Notebooks/Large Scale Machine Learning 2/Final Project'  # Path to project directory, food-101 folder\n",
        "checkpoint_path = os.path.join(project_path, 'checkpoint.pth.tar')\n",
        "torch.save(resnet50_checkpoint, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0.5 marks for answer\n",
        "\n",
        "please, provide detailed plan (and if possible results) of load testing of your service (latency, rps, what services would you use, how would you measure results, how can you track the effect of the model size/speed/quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
